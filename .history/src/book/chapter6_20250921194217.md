---
title: "第6讲：系统工程与高可用部署"
author: [周贤中]
date: 2025-09-04
subject: "Markdown"
keywords: [系统工程, 调度, 监控, 高可用, 可靠性, 配置管理]
lang: zh-cn
---

## 章节总览

从单机多模型到工程化高可用体系：进程与线程模型、调度与优先级、配置与热更新、日志指标监控、故障感知和自愈、版本交付与灰度回滚。核心目标：让推理系统具备“可观察、可控、可自愈、可演进”。

## 部署形态与演进路线

| 阶段 | 形态 | 特征 | 触发升级条件 |
| ---- | ---- | ---- | ------------ |
| POC | 单进程 | 简单，耦合高 | 模型增加/稳定性需求 |
| Beta | 多进程模块化 | 隔离故障 | 资源利用不均/需要扩展 |
| Prod 基础 | 本地 RPC 服务化 | 清晰 API 契约 | 多板协同/多客户端 |
| Prod 进阶 | 容器化 + 编排 | 可滚动更新 | 大规模交付/远程运维 |
| Edge 集群 | 中心调度 + 远程控制 | 全局负载均衡 | 弹性/集中监控 |

进程边界建议：`capture`、`infer`、`postprocess`、`upload`、`monitor`、`watchdog`。隔离崩溃影响并实现差异化资源限额（CPU亲和 + 内存限制）。

## 进程与线程模型设计

### 基本原理

1. 最小可信核心：推理执行逻辑 + 输入输出队列；
2. 外围增强：监控、日志聚合、健康探针不影响核心路径。

### 线程池建议

| 线程组 | 职责 | 数量估算 |
| ------ | ---- | -------- |
| Capture | 采集与解码 | 摄像头数 (N) |
| Preprocess | Resize/Normalize | ceil(N * frame_rate * pre_time / CPU核) |
| Inference | 调用 ACL | 通常 1~2 (避免过度上下文切换) |
| Postprocess | NMS/Decode | 与 Inference 分离防止阻塞 |
| Upload | 事件上报 | 1~2 |
| Monitor | 指标收集 | 1 |

CPU 亲和：将推理线程绑定至高性能核心，避免迁移污染缓存；预处理线程放置在剩余核心以平衡。

## 任务调度与优先级控制

多级队列：RealtimeQueue（最大长度 L1，满则丢弃旧帧）、NormalQueue（批处理）、BackgroundQueue（低优先日志/统计）。
令牌桶限速：对外部请求（远程推理 API）采取令牌桶控制 QPS；令牌不足则延迟或返回限流错误码。
超时策略：当帧在队列停留超过阈值（如 2 × 平均推理时延）标记过期，进入降级路径（丢弃或简化处理）。

## 配置管理与热更新

配置划分：

| 类别 | 内容 | 更新频率 | 是否热更新 |
| ---- | ---- | -------- | ---------- |
| 资源 | 线程数、队列长度 | 低 | 是 |
| 模型 | 路径、版本、精度模式 | 中 | 滚动加载 |
| 策略 | 阈值、降级条件 | 中高 | 是 |
| 安全 | Token、公钥 | 低 | 非热（需重启） |

热更新流程：文件变更 → 校验 schema → 写入新 shadow 副本 → 原子指针切换（正在执行任务继续使用旧配置直至完成）。

## 日志体系与追踪

结构化字段：`ts, level, module, thread, trace_id, latency_ms, event`。
Trace ID：跨进程通过 IPC/RPC header 传递；用于从采集到上报的全链路追踪。
日志级别动态调整：接收管理命令（Unix Domain Socket / 本地控制端口）将模块日志级别置 DEBUG 进行临时诊断。
切割策略：按大小（100MB）或按时间（小时），超限自动压缩归档；保留策略 N 天 + 关键事件永久。

## 指标监控与探针

探针：

- Liveness：进程是否在运行（看门狗检查心跳文件更新时间）。
- Readiness：模型是否加载完成 + 队列是否低压（长度 < 阈值）。
指标暴露格式：`/metrics` Prometheus 文本：`model_latency_bucket{le="..."} 123`。

核心指标分类：

| 分类 | 指标 | 说明 |
| ---- | ---- | ---- |
| 性能 | model_latency_ms (histogram) | 推理时延分位 |
| 吞吐 | frames_processed_total | 每秒增量 |
| 背压 | queue_len / queue_wait_ms | 排队深度 |
| 资源 | npu_util / cpu_util / mem_bytes | 资源利用率 |
| 可靠性 | crash_count / restart_count | 重启频次 |
| 热 | temperature_c | 温度曲线 |
| 质量 | accuracy_drift | 精度回归差异 |

## 高可用与自愈机制

看门狗：子进程每隔 T 秒写心跳文件；超时→发送 SIGTERM→宽限期→SIGKILL→重启并记录事件。

分级降级：

1. 软降级：减小输入分辨率 / 降 FPS / 关闭次要模型；
2. 硬降级：仅保留关键检测模型；
3. 熔断：持续高温或资源不可用 → 暂停推理，仅缓存数据。
状态机：NORMAL → DEGRADED → CRITICAL → RECOVERY → NORMAL。

## 异常分类与处理矩阵

| 类别 | 触发信号 | 初步动作 | 深度动作 | 记录 |
| ---- | -------- | -------- | -------- | ---- |
| 输入 | 空帧/花屏 | 丢弃+计数 | 摄像头重置 | anomaly.log |
| 资源 | OOM 风险 | Dump 内存 | 重建上下文 | memory.log |
| 性能 | P95 飙升 | Profiling on | 降级策略 | perf.log |
| 硬件 | 温度高 | 降载 | 风扇策略/报警 | thermal.log |
| 数据 | 精度偏移 | Dump 样本 | 模型回滚 | quality.log |

## 版本、灰度与回滚

镜像标签：`<model_version>-<git_sha>-<date>`；包含 manifest：模型 hash、配置 hash、构建环境。
灰度策略：按设备集合（Region/Batch）逐步扩大；监控关键指标偏差（时延/Crash）超过阈值立即回滚。
回滚：保留上一稳定版本镜像与配置快照；执行原子 symbolic link 切换。

## 安全与访问控制

最小权限：运行用户无 sudo；只读挂载代码与模型目录，写权限仅日志与缓存路径。
配置签名：管理端生成签名，客户端部署时校验防篡改。
远程指令：白名单 + 签名校验；禁止执行任意 shell。

## 审计与合规

记录：运维操作、配置变更、模型替换、异常重启；保存 JSON Line 格式，便于集中检索。设定留存策略和脱敏规则（剔除用户标识）。

## 示例：两模型多进程结构

```
[capture] -> shm -> [infer_detect] -> mq -> [post_detect]
											  \-> [infer_classify] -> [post_classify]
	 |                                                |
 [monitor] <------------------------------------------
 [watchdog] (supervisor all processes)
```
共享内存（shm）用于高带宽帧传输，消息队列（mq）传递元数据（指针、时间戳、追踪 ID）。

## 章节小结

通过模块化、可观察化与自动化自愈策略，边缘推理系统可以在资源约束与环境不稳定条件下提供接近云端的可靠性。重点：明确边界、度量驱动、降级可逆、版本可控。

## 实践任务

1. 设计多进程与队列拓扑图（ASCII）。
2. 编写队列监控小工具：输出队列长度与平均等待时长。
3. 实现一个看门狗脚本（检测心跳文件时间差 > 阈值则重启模拟进程）。
4. 制作灰度发布计划（分三阶段 + 指标 + 回滚条件）。
5. 输出降级状态机定义（含转移条件）。

